# üéØ COMO USAR O SISTEMA DE TOKENS AGORA

## ‚úÖ O Sistema Est√° Integrado

Todos os posts que forem processados **automaticamente registrar√£o seus tokens REAIS**.

Voc√™ n√£o precisa fazer nada - a captura ocorre transparentemente.

## üìä Ver os Dados de Tokens

### Op√ß√£o 1: Dashboard Interativo (RECOMENDADO)
```bash
python token_logs_viewer.py
```

Oferece 5 op√ß√µes:
1. **Resumo de Hoje** - tokens entrada/sa√≠da de hoje
2. **Por API/Modelo** - dados agrupados por modelo Gemini
3. **Logs Recentes** - √∫ltimos 10 registros detalhados
4. **Compara√ß√£o** - compara per√≠odos (hoje vs semana passada, etc)
5. **Exportar CSV** - salva em arquivo Excel/Calc

### Op√ß√£o 2: Ver arquivos raw
```bash
# Ver o log de hoje (JSONL - uma entrada por linha)
cat logs/tokens/tokens_2026-02-05.jsonl

# Ver estat√≠sticas consolidadas
cat logs/tokens/stats.json
```

## üìà Entender os Dados

### Arquivo JSONL Di√°rio: `logs/tokens/tokens_YYYY-MM-DD.jsonl`

Cada linha √© um registro JSON:
```json
{
  "timestamp": "2026-02-05T12:01:47.125481",
  "prompt_tokens": 150,
  "completion_tokens": 320,
  "total_tokens": 470,
  "api": "gemini",
  "model": "gemini-2.5-flash-lite",
  "metadata": {
    "batch_size": 1,
    "operation": "batch_rewrite"
  },
  "status": "success"
}
```

**O que significa:**
- `prompt_tokens` = Tokens de ENTRADA (seu prompt para a IA)
- `completion_tokens` = Tokens de SA√çDA (resposta da IA)
- `total_tokens` = Soma dos dois
- `operation` = Tipo: `batch_rewrite` (v√°rios posts) ou `single_rewrite` (um post)

### Arquivo Stats: `logs/tokens/stats.json`

Consolida√ß√£o dos √∫ltimos 7 dias:
```json
{
  "date_range": "2026-01-30 to 2026-02-05",
  "total_requests": 8,
  "successful_requests": 8,
  "failed_requests": 0,
  "total_prompt_tokens": 1200,
  "total_completion_tokens": 2540,
  "total_tokens": 3740,
  "average_tokens_per_request": 468,
  "by_model": {
    "gemini-2.5-flash-lite": {
      "requests": 8,
      "prompt_tokens": 1200,
      "completion_tokens": 2540
    }
  }
}
```

## üí∞ Calcular Custo

### Precifica√ß√£o Gemini (Confira valores atuais no console Google Cloud)

Exemplo (valores 2026):
- **gemini-2.5-flash**: $0.075 por 1M entrada | $0.30 por 1M sa√≠da
- **gemini-2.5-flash-lite**: $0.0375 por 1M entrada | $0.15 por 1M sa√≠da

**C√°lculo:**
```
Entrada: 1.200 tokens √ó ($0.0375 / 1.000.000) = $0.000045
Sa√≠da: 2.540 tokens √ó ($0.15 / 1.000.000) = $0.000381
Total: $0.000426 para processar ~2 posts
```

Por m√™s (assumindo 60 posts):
```
60 posts √ó $0.000426 ‚âà $0.026 USD/m√™s
```

## üîÑ Fluxo Completo

```
1. Post processado pelo pipeline
   ‚Üì
2. ai_processor.py chama ai_client_gemini.py
   ‚Üì
3. API Gemini retorna resposta + usage_metadata
   ‚Üì
4. tokens_info extra√≠do: {prompt_tokens: 150, completion_tokens: 320}
   ‚Üì
5. log_tokens() chamado automaticamente
   ‚Üì
6. Dados salvos em:
   - logs/tokens/tokens_2026-02-05.jsonl (entrada di√°ria)
   - logs/tokens/stats.json (consolida√ß√£o)
```

## üîç Troubleshooting

### "N√£o vejo dados novos em logs/tokens/"

1. Verifique se h√° posts sendo processados:
```bash
# Se h√° arquivos novos em debug/ai_response_batch_*.json
Get-ChildItem debug/ai_response_batch*.json -File | Sort-Object LastWriteTime -Descending | Select-Object -First 5
```

2. Confirme integra√ß√£o est√° ok:
```bash
python test_token_integration.py
```

3. Verifique logs da aplica√ß√£o principal:
```bash
tail -f app.log  # ou o arquivo de log que sua app usa
```

### Stats.json n√£o atualiza

- S√≥ atualiza quando h√° dados novos
- Se houver apenas 1 entrada de log, ainda n√£o cria stats
- Execute v√°rios posts para gerar dados agregados

## üìù Exemplo Real

Processando um post sobre filme:
```
Entrada: 312 tokens (seu prompt completo)
Sa√≠da: 645 tokens (resposta HTML + metadata)
Total: 957 tokens

Custo: 957 tokens √ó taxa m√©dia ‚âà $0.00024
```

Para 5 posts similares/dia:
```
5 √ó 957 = 4.785 tokens/dia
‚âà $0.0012/dia
‚âà $0.036/m√™s
```

## üéØ Pr√≥ximos Passos Recomendados

1. **Processe alguns posts** com o pipeline normal
2. **Execute o dashboard**: `python token_logs_viewer.py`
3. **Exporte dados**: Escolha op√ß√£o 5 para CSV
4. **Analise padr√µes**: Qual tipo de post usa mais tokens?
5. **Otimize se necess√°rio**: Instru√ß√µes mais concisas reduzem tokens

---

**Tudo autom√°tico agora! ‚úÖ**

Toda vez que um post √© processado, seus tokens s√£o registrados.
